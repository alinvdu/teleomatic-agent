{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure the following variables based on what you want to generate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "temperature = 1.5 # I found that 1.5 is perfect for generating ideas, increase this for more chaos and\n",
    "# creativity (beware of unintelligible generation). Decrease for more consistency\n",
    "fields = ['Artificial Intelligence', 'Cognitive Psychology', 'Biological Computation',\n",
    "          'Brain Machine Interface'] # modify this list depending on what fields you want to generate ideas\n",
    "\n",
    "# Reference resources for inspiration, papers and articles (pdf and txt format)\n",
    "# Don't forget to add them in Files, on the left side of the screen with drag and drop\n",
    "# Website can be saved as PDF by pressing Print and then save pdf\n",
    "resources = ['entropy-26-00481.pdf',\n",
    "             'Personality-changes.pdf',\n",
    "             'The future of mind.pdf']\n",
    "\n",
    "chunks_limit = 3 # limit on how many chunks we summarize per article, increase\n",
    "# this if you want the LLM to get more inspiration from your articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Installs Unsloth, Xformers (Flash Attention) and all other packages!\n",
    "!pip install unsloth\n",
    "# Get latest Unsloth\n",
    "!pip install --upgrade --no-deps \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length = 8192,\n",
    "    load_in_4bit = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import TextStreamer\n",
    "from unsloth.chat_templates import get_chat_template\n",
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    ")\n",
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "    return text\n",
    "\n",
    "def chunk_text(text, max_chars=2000):\n",
    "    chunks = []\n",
    "    current_pos = 0\n",
    "    while current_pos < len(text):\n",
    "        end_pos = min(current_pos + max_chars, len(text))\n",
    "        chunks.append(text[current_pos:end_pos])\n",
    "        current_pos = end_pos\n",
    "    return chunks\n",
    "\n",
    "def summarize_text_in_idea(chunk):\n",
    "    messages = [\n",
    "        {\"from\": \"system\", \"value\": \"You are a creative summarizer tool for concise and interesting ideas.\"},\n",
    "        {\"from\": \"human\", \"value\": f\"\"\"\n",
    "        Your task is to concisely summarize the following fragment into one or multiple ideas:\n",
    "        {chunk}\n",
    "         Please ignore any metadata such as description about the published articles,\n",
    "         author details and more. Do not provide it as a number list, just plain text \\\n",
    "         delimited by new line.\n",
    "        \"\"\"}\n",
    "    ]\n",
    "\n",
    "    # Create the input IDs (no streamer)\n",
    "    inputs = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=True,\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(\"cuda\")\n",
    "\n",
    "    prompt_len = inputs.shape[-1]\n",
    "\n",
    "    # Generate tokens\n",
    "    output_ids = model.generate(\n",
    "        input_ids=inputs,\n",
    "        max_new_tokens=1024,\n",
    "        use_cache=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "\n",
    "    new_tokens = output_ids[0][prompt_len:]\n",
    "\n",
    "    # Decode the tokens to text\n",
    "    generated_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "def determine_context(resources):\n",
    "  extracted_ideas = []\n",
    "  for pdf_path in resources:\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    chunked_text = chunk_text(text)[:chunks_limit] # we just take the first X chunks to make\n",
    "    # it run faster\n",
    "\n",
    "    for chunk in chunked_text:\n",
    "      curr_idea = summarize_text_in_idea(chunk)\n",
    "      extracted_ideas.append(curr_idea)\n",
    "\n",
    "  return \"\\n\".join(extracted_ideas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "context = determine_context(resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print('The following ideas are generated from the articles and will be used for context:')\n",
    "print(context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "if len(fields) > 1:\n",
    "    concat_fields = ', '.join(fields[:-1]) + ', and ' + fields[-1]\n",
    "else:\n",
    "    concat_fields = fields[0]\n",
    "\n",
    "# system_prompt = f\"\"\"\n",
    "# You are a creative scientist specialized in coming up with groundbreaking, creative \\\n",
    "# ideas in all fields. You have a history of ideas that came to life related to (and \\\n",
    "# not only) psychology, neuroscience, physics, biology, arts, history, artficial intelligence \\\n",
    "# biology.\n",
    "# \"\"\"\n",
    "\n",
    "system_prompt = f\"\"\"\n",
    "You are a creative scientist specialized in purely speculative, theoretical ideas.\n",
    "Below is an example of the style we want:\n",
    "\n",
    "Example style:\n",
    "'Imagine a theoretical framework where AI consciousness arises spontaneously from\n",
    "self-reflection. The system would be more of a philosophical or abstract construct\n",
    "rather than an engineered project...'\n",
    "\n",
    "You are used to generate multiple ideas similar to this.\n",
    "\n",
    "End of example.\n",
    "\"\"\"\n",
    "\n",
    "generator_prompt = f\"\"\"\n",
    "Generate bold, unconventional, and speculative ideas that merge concepts from \\\n",
    "{concat_fields}. Focus on challenging existing assumptions, creating disruptive \\\n",
    "innovations, or proposing entirely new frameworks. Emphasize creativity and \\\n",
    "originality, prioritizing novelty over practicality. Identify emerging, \\\n",
    "underexplored problems or challenges that could benefit from innovative solutions, \\\n",
    "and suggest how your ideas could address them. Not only suggest the idea but also \\\n",
    "provide a framework of how to actually achieve it.\n",
    "\n",
    "Use the following extracted fragments as resources to guide your ideas:\n",
    "{context}\n",
    "\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"from\": \"system\", \"value\": system_prompt},\n",
    "    {\"from\": \"human\", \"value\": generator_prompt}\n",
    "]\n",
    "inputs = tokenizer.apply_chat_template(messages, tokenize = True, add_generation_prompt = True, return_tensors = \"pt\").to(\"cuda\")\n",
    "\n",
    "# Count how many tokens in the prompt\n",
    "prompt_len = inputs.shape[-1]\n",
    "\n",
    "text_streamer = TextStreamer(tokenizer)\n",
    "gen_tokens = model.generate(input_ids = inputs, max_new_tokens = 1024, use_cache = True, temperature=temperature)\n",
    "\n",
    "print(tokenizer.batch_decode(gen_tokens[:, prompt_len:])[0])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
